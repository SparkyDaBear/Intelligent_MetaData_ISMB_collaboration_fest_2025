{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05cb1da7",
   "metadata": {},
   "source": [
    "# Test the Performance of your Models resulting Minimal-Annotation file to the Gold Standard SDRF\n",
    "You will need:\n",
    "1. The full path to the folder containing the predicted minimal-annotation files.  \n",
    "2. The full path to the folder containing the gold standard SDRF files.  \n",
    "3. The full path to the list of allowed annotation types `data/AnnotatedTypes.txt`.  \n",
    "4. The full path to the PRIDE projects summary file `data/pride_projects_summary_20250630.csv`.  \n",
    "5. The full path to the output directory where you want to place your results.  \n",
    "  \n",
    "The files must end in (.ann) and have a file name like follows `PXD003028_PMID27102203.ann`.  \n",
    "    \n",
    "```bash\n",
    "> head data/BenchmarkAnnotations/o4-mini-2025-04-16/PXD005463_PMID28819139.ann \n",
    "Label: stable isotope labeling\n",
    "AssayName: quantitative mass spectrometry\n",
    "Organism: Saccharomyces cerevisiae\n",
    "Strain: YPH499\n",
    "Genotype: Mata, ade2-101, his3-Δ200, leu2-Δ1, ura3-52, trp1-Δ63, and lys2-801\n",
    "Temperature: 24 °C\n",
    "SourceName: YPG medium\n",
    "ConcentrationOfCompound: 1% w/v yeast extract\n",
    "ConcentrationOfCompound: 2% w/v bacto peptone\n",
    "ConcentrationOfCompound: 3% w/v glycerol\n",
    "```\n",
    "Each line is a set of key value pairs where the key is the annotation category and the value is the text span.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23134a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from Levenshtein import distance as lev\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# Default truncation threshold is 1000 elements; lower it to trigger summarization:\n",
    "np.set_printoptions(threshold=np.inf, linewidth=np.inf)\n",
    "\n",
    "##############################################################\n",
    "## STEP 0: Define some user variables\n",
    "## CHANGE THESE TO YOUR OWN PATHS\n",
    "ANN_PATH = f'/storage/group/epo2/default/ims86/git_repos/Intelligent-metadata-compilation/Hackathons_and_challenges/ISMB_collaboration_fest_2025/data/BenchmarkAnnotations/o4-mini-2025-04-16/'\n",
    "\n",
    "SDRF_PATH = f'/storage/group/epo2/default/ims86/git_repos/Intelligent-metadata-compilation/Hackathons_and_challenges/ISMB_collaboration_fest_2025/data/GoldStandard_SDRFs/'\n",
    "\n",
    "ANNTYPES_PATH = f'/storage/group/epo2/default/ims86/git_repos/Intelligent-metadata-compilation/Hackathons_and_challenges/ISMB_collaboration_fest_2025/data/AnnotatedTypes.txt'\n",
    "PRIDE_CSV_PATH = f'/storage/group/epo2/default/ims86/git_repos/Intelligent-metadata-compilation/Hackathons_and_challenges/ISMB_collaboration_fest_2025/data/pride_projects_summary_20250630.csv'\n",
    "\n",
    "OUTPATH = f'/storage/group/epo2/default/ims86/git_repos/Intelligent-metadata-compilation/Hackathons_and_challenges/ISMB_collaboration_fest_2025/data/Performance/Benchmark_Minimal-Annotation_relative_2_GS-SDRF/'\n",
    "\n",
    "# make output directory if it does not exist\n",
    "if not os.path.exists(OUTPATH):\n",
    "    os.makedirs(OUTPATH)\n",
    "\n",
    "\n",
    "# Declare a list of mask strings to ignore when condensing the SDRF data\n",
    "mask_strings = ['not applicable', 'not available',  'not applicable', 'notapplicable', 'n/a', 'na', 'n.a.', 'none', 'no data', 'no data available', 'no data available for this sample', 'no data available for this experiment', 'no data available for this study', 'no data available for this project']\n",
    "\n",
    "# Declare a list of annotation types to ignore when condensing the SDRF data or annotation data\n",
    "ignore_ann_types = ['BiologicalReplicate', 'TechnicalReplicate']\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caac948",
   "metadata": {},
   "source": [
    "# User defined functions  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0125a89a",
   "metadata": {},
   "source": [
    "### Load in the Gold Standard SDRF data\n",
    "The output of this block is a dictionary with the following structure: \n",
    "```\n",
    "cond_sdrf_dict[PMID][AnnType] = [<TextSpan>, <TextSpan>, ..., <TextSpan>]\n",
    "```\n",
    "where AnnType is one of the allowed annotation types in the ANNTYPES_PATH. The text spans are those present in the SDRF for that annotation type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf03e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the SDRF files in the SDRF_PATH\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "#######################################################################################\n",
    "def load_sdrf(SDRF_files:str, condensed_outfile:str, mapping_outfile:str) -> dict:\n",
    "    \"\"\"\n",
    "    Loads the SDRF data\n",
    "    \"\"\"\n",
    "    print(f'\\n{\"#\"*50} Loading SDRF data {\"#\"*50}')\n",
    "\n",
    "    # Load list of valid annotation terms\n",
    "    ValidAnnTypes = np.loadtxt(ANNTYPES_PATH, dtype=str)\n",
    "    print(f'Loaded {len(ValidAnnTypes)} records from ValidAnnotationTypes.csv')\n",
    "    # Filter out the annotation types that are in the ignore list\n",
    "    ValidAnnTypes = [ann_type for ann_type in ValidAnnTypes if ann_type not in ignore_ann_types]\n",
    "    print(f'Filtered out {len(ValidAnnTypes)} annotation types from the ignore list')\n",
    "    print(f'Valid Annotation Types: {ValidAnnTypes}')\n",
    "\n",
    "    # load the dataframe which contains mapping of PXD to PaperID (PMID)\n",
    "    AnnDatasetMeta = pd.read_csv(PRIDE_CSV_PATH, sep='|')\n",
    "    # print(AnnDatasetMeta.head(10))\n",
    "    # print(f'Loaded {len(AnnDatasetMeta)} records from Annotated_dataset_metadata.csv')\n",
    "\n",
    "    # Load the SDRF data into a single dictionary \n",
    "    cond_sdrf_dict = {}\n",
    "    sdrf_mapping = {'PXD':[], 'PMID':[], 'PMCID':[]}  # Dictionary to map PMID to PXD_ID\n",
    "    for sdrf_file in SDRF_files:\n",
    "        print(f'Loading SDRF file: {sdrf_file}')\n",
    "\n",
    "        # Read the SDRF file into a dataframe\n",
    "        sdrf_df = pd.read_csv(sdrf_file, sep='\\t')\n",
    "        print(f'sdrf_df:\\n{sdrf_df.head(10)}')\n",
    "\n",
    "        # get the columns that contain the AnnType\n",
    "        df_columns = sdrf_df.columns\n",
    "        # df_columns = [col.lower() for col in df_columns]  # Convert column names to lowercase\n",
    "        # df_columns = [col.strip() for col in df_columns]\n",
    "        print(f'Columns in SDRF file: {df_columns}')\n",
    "        \n",
    "        # get the PXD ID from the filename\n",
    "        PXD_ID = os.path.basename(sdrf_file).split('.')[0].split('_')[0]\n",
    "        if '-' in PXD_ID:  # handle cases where PXD_ID is split by '-'\n",
    "            PXD_ID = PXD_ID.split('-')[0]\n",
    "        print(f'PXD_ID: {PXD_ID}')\n",
    "        \n",
    "        # get the PaperID from the AnnDatasetMeta dataframe\n",
    "        PXD_df = AnnDatasetMeta[AnnDatasetMeta['accession'] == PXD_ID]\n",
    "        print(f'PXD_df:\\n{PXD_df}')\n",
    "        if PXD_df.empty:\n",
    "            print(f'No records found for {PXD_ID} in AnnDatasetMeta.csv')\n",
    "            continue\n",
    "\n",
    "        PMID = PXD_df['pubmedID'].values[0]\n",
    "        PMC = PXD_df['PMCID'].values[0]\n",
    "        # print(f'PaperID: {PMID} | PMCID: {PMC}')\n",
    "        if PMID not in cond_sdrf_dict: # populate the condensed SDRF dictionary with the paperID as the top level if it does not exist\n",
    "            cond_sdrf_dict[PMID] = {}\n",
    "        \n",
    "        # add them to the mapping dictionary\n",
    "        sdrf_mapping['PXD'].append(PXD_ID)\n",
    "        sdrf_mapping['PMID'].append(PMID)\n",
    "        sdrf_mapping['PMCID'].append(PMC)\n",
    "\n",
    "        # Get the condensed SDRF data for this PaperID\n",
    "        for AnnType in ValidAnnTypes:\n",
    "            print(f'\\nAnnType: {AnnType}')\n",
    "\n",
    "            # check if the AnnType is in the SDRF dataframe\n",
    "            if AnnType not in cond_sdrf_dict[PMID]:\n",
    "                cond_sdrf_dict[PMID][AnnType] = []\n",
    "\n",
    "            # Check if the AnnType is in the columns of the dataframe\n",
    "            # Allow for identification of AnnType.1 or AnnType.2 etc.\n",
    "            # Also allow for AnnType to be in square brackets, e.g. [AnnType]\n",
    "            # This is to handle cases where the AnnType is in the form of [AnnType] or AnnType\n",
    "            # e.g. [BiologicalReplicate] or BiologicalReplicate\n",
    "            # This is to handle cases where the AnnType is in the form of [AnnType].1 or AnnType.1\n",
    "            # e.g. [BiologicalReplicate].1 or BiologicalReplicate.1\n",
    "            AnnType_cols = []\n",
    "            for col in df_columns:\n",
    "                if f'[{AnnType.lower()}]' in col.lower() or f'{AnnType.lower()}' == col.lower():\n",
    "                    AnnType_cols.append(col)\n",
    "                elif f'{AnnType.lower()}.' in col.lower():\n",
    "                    AnnType_cols.append(col)\n",
    "                elif f'{AnnType.lower()} ' in col.lower():\n",
    "                    AnnType_cols.append(col)\n",
    "                elif f'{AnnType.lower()}_' in col.lower():\n",
    "                    AnnType_cols.append(col)\n",
    "                elif f'[{AnnType.lower()}].' in col.lower():\n",
    "                    AnnType_cols.append(col)\n",
    "            print(f'AnnType_cols: {AnnType_cols}')\n",
    "            \n",
    "\n",
    "            # get the rows that contain the AnnType\n",
    "            if len(AnnType_cols) == 0:\n",
    "                # print(f'No columns found for {AnnType}')\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                AnnType_rows = sdrf_df[AnnType_cols]\n",
    "                AnnType_rows = AnnType_rows.astype(str)  # Convert all values to string type\n",
    "                print(f'AnnType_rows:\\n{AnnType_rows} {len(AnnType_rows)} rows')\n",
    "                unique_rows = AnnType_rows.drop_duplicates().values\n",
    "                #unique_rows = [str(row) for row in unique_rows]  # Convert each row to a string\n",
    "                unique_rows = np.hstack(unique_rows)\n",
    "                print(f'Unique AnnType_rows:\\n{unique_rows} {len(unique_rows)} {unique_rows.shape} rows')\n",
    "\n",
    "                # check if any string in the unique_rows as \"NT=\" in it\n",
    "                cleaned_rows = []\n",
    "                for row in unique_rows:\n",
    "                    if row in mask_strings:  # Ignore the strings that are in the mask_strings list\n",
    "                        continue\n",
    "\n",
    "                    if 'NT=' in row:\n",
    "                        # print(f'Found NT= in row: {row}')\n",
    "                        row = row.split(';')  # Split the row by ';'\n",
    "                        row = [r for r in row if 'NT=' in r]  # Keep only strings that contain 'NT='\n",
    "                        row = row[0].replace('NT=', '')  # Remove 'NT=' from the first string\n",
    "                        # print(f'Cleaned row: {row}')\n",
    "                        cleaned_rows.append(row)\n",
    "                    else:\n",
    "                        cleaned_rows.append(row.strip())\n",
    "\n",
    "                print(f'Cleaned rows: {cleaned_rows} {len(cleaned_rows)} rows')\n",
    "                cond_sdrf_dict[PMID][AnnType] += cleaned_rows\n",
    "\n",
    "    # Log data so we can quality check to see if the SDRF data is correct\n",
    "    for paper in cond_sdrf_dict.keys():\n",
    "        for AnnType in cond_sdrf_dict[paper].keys():\n",
    "            #cond_sdrf_dict[paper][AnnType] = list(set(cond_sdrf_dict[paper][AnnType]))\n",
    "            print(f'PaperID: {paper} | AnnType: {AnnType} | {cond_sdrf_dict[paper][AnnType]}')\n",
    "\n",
    "    # save the condensed SDRF data to a pkl file\n",
    "    # condensed_outfile = os.path.join(OUTPATH, 'condensed_sdrf_data.pkl')\n",
    "    with open(condensed_outfile, 'wb') as f:\n",
    "        np.save(f, cond_sdrf_dict, allow_pickle=True)\n",
    "    print(f'Wrote {len(cond_sdrf_dict)} records to {condensed_outfile}')\n",
    "\n",
    "    # save the mapping dictionary to a csv file\n",
    "    # mapping_outfile = os.path.join(OUTPATH, 'sdrf_mapping.csv')\n",
    "    mapping_df = pd.DataFrame(sdrf_mapping)\n",
    "    print(f'mapping_df:\\n{mapping_df}')\n",
    "    mapping_df.to_csv(mapping_outfile, index=False)\n",
    "    print(f'Wrote {len(sdrf_mapping)} records to {mapping_outfile}')\n",
    "\n",
    "    return cond_sdrf_dict, sdrf_mapping\n",
    "#######################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b152807",
   "metadata": {},
   "source": [
    "### Load annotations\n",
    "The output of this block is a dictionary with the following structure: \n",
    "```\n",
    "ann_data_dict[PMID][AnnType] = [<TextSpan>, <TextSpan>, ..., <TextSpan>]\n",
    "```\n",
    "where AnnType is one of the allowed annotation types in the ANNTYPES_PATH. The text spans are those present in the publication for that annotation type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2231582",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#######################################################################################\n",
    "def load_annotations(ann_files, ann_outfile, mapping_outfile):\n",
    "    \"\"\"\n",
    "    Loads all the annotations into a single dataframe for easy analysis\n",
    "    \"\"\"\n",
    "    print(f'\\n{\"#\"*50} Loading Annotations {\"#\"*50}')\n",
    "\n",
    "    # Load list of valid annotation terms\n",
    "    ValidAnnTypes = np.loadtxt(ANNTYPES_PATH, dtype=str)\n",
    "    print(f'Loaded {len(ValidAnnTypes)} records from ValidAnnotationTypes.csv')\n",
    "    # Filter out the annotation types that are in the ignore list\n",
    "    ValidAnnTypes = [ann_type for ann_type in ValidAnnTypes if ann_type not in ignore_ann_types]\n",
    "    print(f'Filtered out {len(ValidAnnTypes)} annotation types from the ignore list')\n",
    "    print(f'Valid Annotation Types: {ValidAnnTypes}')\n",
    "\n",
    "    # load the dataframe which contains mapping of PXD to PaperID (PMID)\n",
    "    AnnDatasetMeta = pd.read_csv(PRIDE_CSV_PATH, sep='|')\n",
    "    # print(AnnDatasetMeta.head(10))\n",
    "    # print(f'Loaded {len(AnnDatasetMeta)} records from Annotated_dataset_metadata.csv')\n",
    "\n",
    "    # For each ann_file load its contents into a dictionary\n",
    "    ann_data_dict = {}\n",
    "    ann_mapping = {'PXD':[], 'PMID':[], 'PMCID':[]}  # Dictionary to map PMID to PXD_ID\n",
    "    for ann_file in ann_files:\n",
    "        print(f'Loading annotation file: {ann_file}')\n",
    "\n",
    "        \n",
    "        PXD_ID = os.path.basename(ann_file).split('.')[0].split('_')[0]\n",
    "        if '-' in PXD_ID:  # handle cases where PXD_ID is split by '-'\n",
    "            PXD_ID = PXD_ID.split('-')[0]\n",
    "\n",
    "        # get the PaperID from the AnnDatasetMeta dataframe\n",
    "        PXD_df = AnnDatasetMeta[AnnDatasetMeta['accession'] == PXD_ID]\n",
    "        # print(f'PXD_df:\\n{PXD_df}')\n",
    "        if PXD_df.empty:\n",
    "            print(f'No records found for {PXD_ID} in AnnDatasetMeta.csv')\n",
    "            continue\n",
    "\n",
    "        PMID = os.path.basename(ann_file).split('.')[0].split('_')[1]\n",
    "        if '-' in PMID:  # handle cases where PMID is split by '-'\n",
    "            PMID = PMID.split('-')[0]\n",
    "        PMID = PMID.replace('PMID', '')  # remove 'PMID' prefix if it exists\n",
    "\n",
    "        PMID = PXD_df['pubmedID'].values[0]\n",
    "        PMC = PXD_df['PMCID'].values[0]\n",
    "\n",
    "        # add them to the mapping dictionary\n",
    "        ann_mapping['PXD'].append(PXD_ID)\n",
    "        ann_mapping['PMID'].append(PMID)\n",
    "        ann_mapping['PMCID'].append(PMC)\n",
    "        # print(f'PXD_ID: {PXD_ID} | PMID: {PMID}')\n",
    "\n",
    "        if PMID not in ann_data_dict: # populate the condensed SDRF dictionary with the paperID as the top level if it does not exist\n",
    "            ann_data_dict[PMID] = {}\n",
    "\n",
    "        # Load the annotation file into a dictionary\n",
    "        data = {}\n",
    "        with open(ann_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if ':' in line:  # ensure the line contains a key-value pair\n",
    "                    # print(f'LINE: {line}')\n",
    "                    key = line.split(': ')[0].strip()\n",
    "                    value = line.split(': ')[1].strip()\n",
    "                    data[key] = value\n",
    "\n",
    "\n",
    "        # Get the condensed SDRF data for this PaperID\n",
    "        for AnnType in ValidAnnTypes:\n",
    "            # print(f'\\nAnnType: {AnnType}')\n",
    "\n",
    "            # check if the AnnType is in the SDRF dataframe\n",
    "            if AnnType not in ann_data_dict[PMID]:\n",
    "                ann_data_dict[PMID][AnnType] = [v for k,v in data.items() if AnnType.lower() == k.lower()]\n",
    "\n",
    "    # Log data so we can quality check to see if the SDRF data is correct\n",
    "    for paper in ann_data_dict.keys():\n",
    "        for AnnType in ann_data_dict[paper].keys():\n",
    "            #ann_data_dict[paper][AnnType] = list(set(ann_data_dict[paper][AnnType]))\n",
    "            print(f'PaperID: {paper} | AnnType: {AnnType} | {ann_data_dict[paper][AnnType]}')\n",
    "\n",
    "    # save the condensed annotation data to a pkl file\n",
    "    with open(ann_outfile, 'wb') as f:\n",
    "        np.save(f, ann_data_dict, allow_pickle=True)\n",
    "    print(f'Wrote {len(ann_data_dict)} records to {ann_outfile}')\n",
    "\n",
    "    # save the mapping dictionary to a csv file\n",
    "    mapping_df = pd.DataFrame(ann_mapping)\n",
    "    print(f'mapping_df:\\n{mapping_df}')\n",
    "    mapping_df.to_csv(mapping_outfile, index=False)\n",
    "    print(f'Wrote {len(ann_mapping)} records to {mapping_outfile}')\n",
    "\n",
    "    return ann_data_dict, ann_mapping \n",
    "#######################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63549db1",
   "metadata": {},
   "source": [
    "### Get the frequency of each annotation type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e548a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "def AnnotationTypeFrequency(data: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Computes the frequency distribution of annotation types in the dataset.\n",
    "    \"\"\"\n",
    "    print(f'\\n{\"#\"*50} Computing Annotation Type Frequencies {\"#\"*50}')\n",
    "\n",
    "    # Initialize a dictionary to hold the frequency distributions\n",
    "    freq_dists = {}\n",
    "\n",
    "    # Compute the frequency distribution for each paper\n",
    "    for paper_id, annotations in data.items():\n",
    "        # Count the occurrences of each annotation type\n",
    "        for ann_type, ann_values in annotations.items():\n",
    "            if ann_type not in freq_dists:\n",
    "                freq_dists[ann_type] = 0\n",
    "            freq_dists[ann_type] += len(ann_values)\n",
    "\n",
    "    return freq_dists\n",
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dbe5be",
   "metadata": {},
   "source": [
    "### Harmonize and Evaluate the Performance and Consistency Between Annotation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5490fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "# Default truncation threshold is 1000 elements; lower it to trigger summarization:\n",
    "np.set_printoptions(threshold=np.inf, linewidth=np.inf)\n",
    "\n",
    "##################################################################################\n",
    "def Harmonize_and_Evaluate_datasets( \n",
    "    A: Dict[str, Dict[str, List[str]]],\n",
    "    B: Dict[str, Dict[str, List[str]]],\n",
    "    Aoutfile: str = 'harmonized_A.pkl',\n",
    "    Boutfile: str = 'harmonized_B.pkl',\n",
    "    eval_outfile: str = 'evaluation_metrics.csv',\n",
    "    threshold: float = 0.80,\n",
    "    method: str = 'RapidFuzz',  # 'Levenshtein' or 'Embedding' or 'RapidFuzz'\n",
    "    CompleteAbsence: float = float('NaN') \n",
    "    ) -> Tuple[\n",
    "    Dict[str, Dict[str, List[str]]],  # harmonized A\n",
    "    Dict[str, Dict[str, List[str]]]  # harmonized B\n",
    "    ]:\n",
    "\n",
    "    model_name = 'all-MiniLM-L6-v2'\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    ## Get the common set of top level keys\n",
    "    print(A.keys())\n",
    "    print(B.keys())\n",
    "    common_pubs = set(A.keys()).intersection(set(B.keys()))\n",
    "    print(f'Found {len(common_pubs)} common publications between A and B datasets.')\n",
    "\n",
    "    # raise SystemExit(\"Please run the harmonization process with the correct datasets before evaluating.\")\n",
    "    harmonized_A = {}\n",
    "    harmonized_B = {}\n",
    "    eval_metrics = {'publication': [], 'AnnotationType': [], 'precision': [], 'recall': [], 'f1': [], 'jacc': []}\n",
    "    for pub in common_pubs:\n",
    "        harmonized_A[pub] = {}\n",
    "        harmonized_B[pub] = {}\n",
    "        for category in A[pub].keys():\n",
    "            vals_A = list(set(A[pub][category]))\n",
    "            vals_B = list(set(B[pub][category]))\n",
    "            all_vals = vals_A + [v for v in vals_B if v not in vals_A]\n",
    "            print(f'\\nProcessing {pub} - {category}: {len(all_vals)} unique values')\n",
    "            print(f'vals_A: {vals_A} | vals_B: {vals_B} | all_vals: {all_vals}')\n",
    "\n",
    "            # 1a. auto return empty if there is no annotations for set A or B \n",
    "            if len(vals_A) == 0 and len(vals_B) == 0:\n",
    "                print(f'No values found for {pub} - {category}.')\n",
    "                harmA = []\n",
    "                harmB = []\n",
    "                harmonized_A[pub][category] = harmA\n",
    "                harmonized_B[pub][category] = harmB\n",
    "                print(f'Harmonized A: {harmA}')\n",
    "                print(f'Harmonized B: {harmB}')\n",
    "                eval_metrics['publication'].append(pub)\n",
    "                eval_metrics['AnnotationType'].append(category)\n",
    "                eval_metrics['precision'].append(CompleteAbsence)\n",
    "                eval_metrics['recall'].append(CompleteAbsence)\n",
    "                eval_metrics['f1'].append(CompleteAbsence)\n",
    "                eval_metrics['jacc'].append(CompleteAbsence)\n",
    "                continue\n",
    "\n",
    "            # handle case where there is only a single value in the all_vals list\n",
    "            if len(all_vals) == 1:\n",
    "                dist_mat = np.array([[0.0]])  # single element, distance to itself is 0\n",
    "                labels = np.array([0])  # single cluster\n",
    "                print(f'Only one unique value for {pub} - {category}. Skipping clustering.')\n",
    "\n",
    "            else:\n",
    "                \n",
    "                if method == 'Embedding':\n",
    "                    print(f'Using embeddings for {pub} - {category} with threshold {threshold}')\n",
    "                    # if more than 1 value compute embeddings\n",
    "                    embeddings = model.encode(all_vals, convert_to_numpy=True, normalize_embeddings=True)\n",
    "                    print(f'embeddings shape: {embeddings.shape}')\n",
    "\n",
    "                    # 1c. cosine similarity → distance matrix\n",
    "                    sim_mat = cosine_similarity(embeddings)                # shape (N, N)\n",
    "                    # print(f'sim_mat shape: {sim_mat.shape}')\n",
    "                    # print(f'sim_mat:\\n{sim_mat}')\n",
    "                    dist_mat = 1.0 - sim_mat                        # distance in [0, 2]\n",
    "                    print(f'dist_mat shape: {dist_mat.shape}')\n",
    "                    print(f'dist_mat:\\n{dist_mat}')\n",
    "\n",
    "                elif method == 'Levenshtein':\n",
    "                    print(f'Using Levenshtein distance for {pub} - {category} with threshold {threshold}')\n",
    "                    # if more than 1 value compute Levenshtein distance\n",
    "                    dist_mat = np.zeros((len(all_vals), len(all_vals)), dtype=float)\n",
    "                    for i in range(len(all_vals)):\n",
    "                        for j in range(i + 1, len(all_vals)):\n",
    "                            dist = lev(all_vals[i], all_vals[j])\n",
    "                            dist = dist / max(len(all_vals[i]), len(all_vals[j]))  # normalize distance\n",
    "                            dist_mat[i, j] = dist\n",
    "                            dist_mat[j, i] = dist\n",
    "                    print(f'dist_mat shape: {dist_mat.shape}')\n",
    "                    print(f'dist_mat:\\n{dist_mat}')\n",
    "\n",
    "                elif method == 'RapidFuzz':\n",
    "                    print(f'Using RapidFuzz distance for {pub} - {category} with threshold {threshold}')\n",
    "                    # if more than 1 value compute RapidFuzz distance\n",
    "                    dist_mat = np.zeros((len(all_vals), len(all_vals)), dtype=float)\n",
    "                    for i in range(len(all_vals)):\n",
    "                        for j in range(i + 1, len(all_vals)):\n",
    "                            dist = fuzz.ratio(all_vals[i], all_vals[j]) / 100.0  # normalize to [0, 1]\n",
    "                            dist_mat[i, j] = 1.0 - dist  # convert to distance\n",
    "                            dist_mat[j, i] = dist_mat[i, j]\n",
    "                    print(f'dist_mat shape: {dist_mat.shape}')\n",
    "                    print(f'dist_mat:\\n{dist_mat}')\n",
    "\n",
    "                else:\n",
    "                    raise ValueError(f'Unknown method: {method}. Use \"Embedding\" or \"Levenshtein\".')\n",
    "\n",
    "                # 1d. cluster\n",
    "                clusterer = AgglomerativeClustering(\n",
    "                    n_clusters=None,\n",
    "                    metric='precomputed',\n",
    "                    linkage='average',\n",
    "                    distance_threshold=1.0 - threshold)\n",
    "                labels = clusterer.fit_predict(dist_mat)        # array of length N\n",
    "\n",
    "            # map each string → cluster\n",
    "            str2cid = {s: int(labels[i]) for i, s in enumerate(all_vals)}\n",
    "            print(f'str2cid: {str2cid}')\n",
    "\n",
    "            # 1e. harmonize original lists\n",
    "            harmA = [str2cid[s] for s in A[pub][category]]\n",
    "            harmB = [str2cid[s] for s in B[pub][category]]\n",
    "            print(f'Harmonized A: {harmA}')\n",
    "            print(f'Harmonized B: {harmB}')\n",
    "\n",
    "            harmonized_A[pub][category] = harmA\n",
    "            harmonized_B[pub][category] = harmB\n",
    "\n",
    "            # Else, proceed with evaluation\n",
    "            print(f'\\nEvaluating - {category}:')\n",
    "            print(f'y_true: {harmA}')\n",
    "            print(f'y_pred: {harmB}')\n",
    "\n",
    "            unique_labels = set(harmA + harmB)\n",
    "            print(f'Unique labels: {unique_labels}')\n",
    "\n",
    "            y_true_p = []\n",
    "            y_pred_p = []\n",
    "            for label in unique_labels:\n",
    "                if label in harmA:\n",
    "                    y_true_p.append(1)\n",
    "                else:\n",
    "                    y_true_p.append(0)\n",
    "                if label in harmB:\n",
    "                    y_pred_p.append(1)\n",
    "                else:\n",
    "                    y_pred_p.append(0)\n",
    "            print(f'y_true_p: {y_true_p}')\n",
    "            print(f'y_pred_p: {y_pred_p}')\n",
    "\n",
    "            precision = precision_score(y_true_p, y_pred_p, average='macro', zero_division=0)\n",
    "            recall    = recall_score(y_true_p, y_pred_p, average='macro', zero_division=0)\n",
    "            f1        = f1_score(y_true_p, y_pred_p, average='macro', zero_division=0)\n",
    "            print(f'Precision: {precision}, Recall: {recall}, F1: {f1}')\n",
    "        \n",
    "            # consistency: Jaccard over the sets of cluster IDs\n",
    "            set_A = set(harmonized_A[pub][category])\n",
    "            set_B = set(harmonized_B[pub][category])\n",
    "            if not set_A and not set_B:\n",
    "                jacc = 1.0\n",
    "            else:\n",
    "                # compute |A ∩ B| / |A ∪ B|\n",
    "                jacc = len(set_A & set_B) / len(set_A | set_B)\n",
    "            print(f'Jaccard consistency for {pub} - {category}: {jacc}')\n",
    "\n",
    "            # Store the evaluation metrics\n",
    "            eval_metrics['publication'].append(pub)\n",
    "            eval_metrics['AnnotationType'].append(category)\n",
    "            eval_metrics['precision'].append(precision)\n",
    "            eval_metrics['recall'].append(recall)\n",
    "            eval_metrics['f1'].append(f1)\n",
    "            eval_metrics['jacc'].append(jacc)\n",
    "\n",
    "    # Convert the evaluation metrics to a DataFrame\n",
    "    eval_df = pd.DataFrame(eval_metrics)\n",
    "    print(f'\\nEvaluation Metrics:\\n{eval_df}')\n",
    "    eval_df.to_csv(eval_outfile, index=False)\n",
    "    print(f'Wrote evaluation metrics to {eval_outfile}')\n",
    "\n",
    "    # save the harmonized datasets\n",
    "    with open(Aoutfile, 'wb') as f:\n",
    "        np.save(f, harmonized_A, allow_pickle=True)\n",
    "    print(f'Wrote {len(harmonized_A)} records to {Aoutfile}')\n",
    "\n",
    "    with open(Boutfile, 'wb') as f:\n",
    "        np.save(f, harmonized_B, allow_pickle=True)\n",
    "    print(f'Wrote {len(harmonized_B)} records to {Boutfile}')\n",
    "    \n",
    "    # return the harmonized datasets\n",
    "    print(f'Harmonization complete. Returning harmonized datasets.')\n",
    "    return harmonized_A, harmonized_B, eval_df\n",
    "##################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762f3c6c",
   "metadata": {},
   "source": [
    "### Plot Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a63ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "def PlotEvaluationMetrics(eval_df: pd.DataFrame, outpath: str = 'evaluation_metrics_plot', title: str = 'Evaluation Metrics by Annotation Type') -> None:\n",
    "    \"\"\"\n",
    "    Plots the evaluation metrics from the evaluation DataFrame.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from scipy.stats import bootstrap\n",
    "\n",
    "    #######################################################################\n",
    "    print(f'\\n{\"#\"*50} Summarizing Evaluation Metrics {\"#\"*50}')\n",
    "    csv_outpath = outpath.split('.')[0] + '.csv'\n",
    "    plot_df = {'AnnotationType': [], 'precision': [], 'precision_lb': [], 'precision_ub':[], \n",
    "                                      'recall': [], 'recall_lb': [], 'recall_ub':[], \n",
    "                                      'f1': [], 'f1_lb': [], 'f1_ub':[], \n",
    "                                      'jacc': [], 'jacc_lb': [], 'jacc_ub':[], }\n",
    "    for AnnType, AnnType_df in eval_df.groupby('AnnotationType'):\n",
    "        print(f'Summarizing metrics for annotation type: {AnnType}')\n",
    "        print(f'AnnType_df:\\n{AnnType_df}')\n",
    "\n",
    "        # remove rows with NaN values in the metrics\n",
    "        AnnType_df = AnnType_df.dropna(subset=['precision', 'recall', 'f1', 'jacc'])\n",
    "        print(f'AnnType_df after dropping NaN values:\\n{AnnType_df}')\n",
    "\n",
    "        # If there are no rows left after dropping NaN values, skip this annotation type\n",
    "        if AnnType_df.empty:\n",
    "            print(f'No valid data for {AnnType}. Skipping.')\n",
    "            continue\n",
    "\n",
    "        # Append the metrics to the plot DataFrame\n",
    "        plot_df['AnnotationType'].append(AnnType)\n",
    "\n",
    "        if len(AnnType_df) == 1:\n",
    "            # If there is only one row, use the values directly\n",
    "            plot_df['precision'].append(AnnType_df['precision'].values[0])\n",
    "            plot_df['precision_lb'].append(AnnType_df['precision'].values[0])\n",
    "            plot_df['precision_ub'].append(AnnType_df['precision'].values[0])\n",
    "            plot_df['recall'].append(AnnType_df['recall'].values[0])\n",
    "            plot_df['recall_lb'].append(AnnType_df['recall'].values[0])\n",
    "            plot_df['recall_ub'].append(AnnType_df['recall'].values[0])\n",
    "            plot_df['f1'].append(AnnType_df['f1'].values[0])\n",
    "            plot_df['f1_lb'].append(AnnType_df['f1'].values[0])\n",
    "            plot_df['f1_ub'].append(AnnType_df['f1'].values[0])\n",
    "            plot_df['jacc'].append(AnnType_df['jacc'].values[0])\n",
    "            plot_df['jacc_lb'].append(AnnType_df['jacc'].values[0])\n",
    "            plot_df['jacc_ub'].append(AnnType_df['jacc'].values[0])\n",
    "            print(f'Only one row for {AnnType}. Using the values directly.')\n",
    "            continue\n",
    "        \n",
    "        # Calculate the mean and 95% confidence intervals for each metric\n",
    "        precision_mean = AnnType_df['precision'].mean()\n",
    "        precision_ci = bootstrap((AnnType_df['precision'].values,), np.mean, confidence_level=0.95, n_resamples=10000)\n",
    "        precision_lb = precision_ci.confidence_interval.low\n",
    "        precision_ub = precision_ci.confidence_interval.high\n",
    "\n",
    "        recall_mean = AnnType_df['recall'].mean()\n",
    "        recall_ci = bootstrap((AnnType_df['recall'].values,), np.mean, confidence_level=0.95, n_resamples=10000)\n",
    "        recall_lb = recall_ci.confidence_interval.low\n",
    "        recall_ub = recall_ci.confidence_interval.high\n",
    "\n",
    "        f1_mean = AnnType_df['f1'].mean()\n",
    "        f1_ci = bootstrap((AnnType_df['f1'].values,), np.mean, confidence_level=0.95, n_resamples=10000)\n",
    "        f1_lb = f1_ci.confidence_interval.low\n",
    "        f1_ub = f1_ci.confidence_interval.high\n",
    "\n",
    "        jacc_mean = AnnType_df['jacc'].mean()\n",
    "        jacc_ci = bootstrap((AnnType_df['jacc'].values,), np.mean, confidence_level=0.95, n_resamples=10000)\n",
    "        jacc_lb = jacc_ci.confidence_interval.low\n",
    "        jacc_ub = jacc_ci.confidence_interval.high\n",
    "\n",
    "        # Append the metrics to the plot DataFrame\n",
    "        plot_df['precision'].append(precision_mean)\n",
    "        plot_df['precision_lb'].append(precision_lb)\n",
    "        plot_df['precision_ub'].append(precision_ub)\n",
    "        plot_df['recall'].append(recall_mean)\n",
    "        plot_df['recall_lb'].append(recall_lb)\n",
    "        plot_df['recall_ub'].append(recall_ub)\n",
    "        plot_df['f1'].append(f1_mean)\n",
    "        plot_df['f1_lb'].append(f1_lb)\n",
    "        plot_df['f1_ub'].append(f1_ub)\n",
    "        plot_df['jacc'].append(jacc_mean)\n",
    "        plot_df['jacc_lb'].append(jacc_lb)\n",
    "        plot_df['jacc_ub'].append(jacc_ub)\n",
    "\n",
    "    plot_df = pd.DataFrame(plot_df)\n",
    "    print(f'Plot DataFrame:\\n{plot_df}')\n",
    "    # Save the plot DataFrame to a CSV file\n",
    "    plot_df.to_csv(csv_outpath, index=False)\n",
    "    print(f'Saved evaluation metrics to {csv_outpath}')\n",
    "    #######################################################################\n",
    "\n",
    "    #######################################################################\n",
    "    print(f'\\n{\"#\"*50} Plotting Evaluation Metrics {\"#\"*50}')\n",
    "    # assume your DataFrame is called df\n",
    "    # set AnnotationType as the index\n",
    "    df = plot_df.set_index('AnnotationType')\n",
    "\n",
    "    # the metrics to plot\n",
    "    metrics = ['precision', 'recall', 'f1', 'jacc']\n",
    "\n",
    "    # build a matrix of annotation strings:\n",
    "    # each row is [“val (lb, ub)” for each metric]\n",
    "    annot = df.apply(\n",
    "        lambda row: [\n",
    "            f\"{row[m]:.3f} ({row[f'{m}_lb']:.3f}, {row[f'{m}_ub']:.3f})\"\n",
    "            for m in metrics\n",
    "        ],\n",
    "        axis=1\n",
    "    ).tolist()\n",
    "\n",
    "    # convert to a NumPy array for sns.heatmap\n",
    "    annot = np.array(annot)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, len(df)*0.25))  # adjust height per row\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    ax = sns.heatmap(\n",
    "        df[metrics].astype(float),\n",
    "        annot=annot,\n",
    "        fmt=\"\",\n",
    "        cmap=\"Blues\",\n",
    "        cbar=True,  # <-- ENABLE COLOR BAR\n",
    "        cbar_kws={\"label\": \"Score\"},  # <-- OPTIONAL: Add label\n",
    "        linewidths=0.5,\n",
    "        linecolor=\"lightgray\"\n",
    "    )\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_xticklabels(metrics, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(df.index, rotation=0)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath, dpi=300)\n",
    "    print(f'Saved evaluation metrics plot to {outpath}')\n",
    "    plt.show()\n",
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73880e4a",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2e7c86",
   "metadata": {},
   "source": [
    "## Load Data from SDRF and Minimal annotation files  \n",
    "Loads data from the SDRF files (Gold standard human curated) and the results of a model annotating the same text provided in the minimal annotation format described at the top of this notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019be972",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "### STEP 1: Load the SDRF data ###############################################\n",
    "# Load the SDRF data\n",
    "# This will create a dictionary with the following structure:\n",
    "# sdrf_data[PMID][AnnType] = [<TextSpan>, <TextSpan>, ..., <TextSpan>]\n",
    "# where AnnType is one of the allowed annotation types in the ANNTYPES_PATH.\n",
    "# get the list of SDRF files\n",
    "# Find the SDRF files in the SDRF_PATH\n",
    "sdrf_files = [os.path.join(SDRF_PATH, f) for f in os.listdir(SDRF_PATH) if f.endswith('_cleaned.sdrf.tsv')]\n",
    "print(f\"Found {len(sdrf_files)} SDRF files in {SDRF_PATH}\")\n",
    "\n",
    "# Check if the condensed SDRF data already exists\n",
    "condensed_outfile = os.path.join(OUTPATH, 'condensed_sdrf_data.pkl')\n",
    "condensed_mapping_outfile = os.path.join(OUTPATH, 'sdrf_mapping.csv')\n",
    "if os.path.exists(condensed_outfile) and os.path.exists(condensed_mapping_outfile):\n",
    "    print(f'Condensed SDRF data already exists at {condensed_outfile}. Loading existing data.')\n",
    "\n",
    "    with open(condensed_outfile, 'rb') as f:\n",
    "        sdrf_data = np.load(f, allow_pickle=True).item()\n",
    "    print(f'Loaded {len(sdrf_data)} records from {condensed_outfile}')\n",
    "\n",
    "    sdrf_mapping = pd.read_csv(condensed_mapping_outfile)\n",
    "    print(f'Loaded {len(sdrf_mapping)} records from {condensed_mapping_outfile}')\n",
    "\n",
    "else:\n",
    "    print(f'Condensed SDRF data does not exist. Creating new data at {condensed_outfile}.')\n",
    "    sdrf_data, sdrf_mapping = load_sdrf(sdrf_files, condensed_outfile, condensed_mapping_outfile)\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "### STEP 2: Load the annotation data #########################################\n",
    "# Load the annotations into a single dictionary\n",
    "# This will create a dictionary with the following structure:\n",
    "# ann_data_dict[PMID][AnnType] = [<TextSpan>, <TextSpan>, ..., <TextSpan>]\n",
    "# where AnnType is one of the allowed annotation types in the ANNTYPES_PATH.\n",
    "# The text spans are those present in the publication for that annotation type.\n",
    "# get the list of annotation files\n",
    "ann_files = glob.glob(os.path.join(ANN_PATH, '*.ann'))\n",
    "print(f\"Found {len(ann_files)} annotation files in {ANN_PATH}\")\n",
    "\n",
    "# Load the annotations into a single dictionary\n",
    "ann_outfile = os.path.join(OUTPATH, 'condensed_ann_data.pkl')\n",
    "ann_mapping_outfile = os.path.join(OUTPATH, 'ann_mapping.csv')\n",
    "if os.path.exists(ann_outfile) and os.path.exists(ann_mapping_outfile):\n",
    "    print(f'Condensed annotation data already exists at {ann_outfile}. Loading existing data.')\n",
    "\n",
    "    with open(ann_outfile, 'rb') as f:\n",
    "        ann_data = np.load(f, allow_pickle=True).item()\n",
    "    print(f'Loaded {len(ann_data)} records from {ann_outfile}')\n",
    "\n",
    "    ann_mapping = pd.read_csv(ann_mapping_outfile)\n",
    "    print(f'Loaded {len(ann_mapping)} records from {ann_mapping_outfile}')\n",
    "\n",
    "else:\n",
    "    print(f'Condensed annotation data does not exist. Creating new data at {ann_outfile}.')\n",
    "    ann_data, ann_mapping = load_annotations(ann_files, ann_outfile, ann_mapping_outfile)\n",
    "##############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d812daed",
   "metadata": {},
   "source": [
    "## Compare Annotation Type Frequency  \n",
    "Calculate the per annotation type frequncy for the SDRF (Gold Standard) and the Minimal Annotation (test data) to determine if you are over or under annotating certain categories.  \n",
    "This calculates the number of unique annotation labels in each type so in the SDRF if there are multiple samples with the same organism part it will only count once.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db8febd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the frequency for each annotation type \n",
    "# This will create a dictionary with the following structure:\n",
    "# Ann_freq_dists[AnnType] = frequency\n",
    "# where AnnType is one of the allowed annotation types in the ANNTYPES_PATH.\n",
    "# and frequency is the number of occurrences of that annotation type in the dataset.\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "annotation_freq_outfile = os.path.join(OUTPATH, 'annotation_type_frequencies.csv')\n",
    "if os.path.exists(annotation_freq_outfile):\n",
    "    print(f'Annotation type frequencies already exist at {annotation_freq_outfile}. Loading existing data.')\n",
    "    freq_df = pd.read_csv(annotation_freq_outfile)\n",
    "\n",
    "else:\n",
    "    print(f'Annotation type frequencies do not exist. Creating new data at {annotation_freq_outfile}.')\n",
    "    # Get the frequency for the user supplied annotation data\n",
    "    Ann_freq_dists = AnnotationTypeFrequency(ann_data)\n",
    "    print(f'Annotation frequency distributions: {Ann_freq_dists}')\n",
    "\n",
    "    # Get the frequency for the SDRF data\n",
    "    Sdrf_freq_dists = AnnotationTypeFrequency(sdrf_data)\n",
    "    print(f'SDRF frequency distributions: {Sdrf_freq_dists}')\n",
    "\n",
    "    # make a dataframe with the frequency distributions\n",
    "    freq_df = pd.DataFrame({\n",
    "        'Annotation Type': list(Ann_freq_dists.keys()),\n",
    "        'Annotation Frequency': list(Ann_freq_dists.values()),\n",
    "        'SDRF Frequency': [Sdrf_freq_dists.get(ann_type, 0) for ann_type in Ann_freq_dists.keys()]\n",
    "    })\n",
    "    # freq_df = freq_df.set_index('Annotation Type')\n",
    "    # save the frequency dataframe to a csv file\n",
    "    freq_df.to_csv(annotation_freq_outfile, index=False)\n",
    "    print(f'Wrote {len(freq_df)} records to {annotation_freq_outfile}')\n",
    "    \n",
    "annotation_freq_png_outfile = os.path.join(OUTPATH, 'annotation_type_frequencies.png')\n",
    "# Print the frequency dataframe\n",
    "IGNORE_COLS = ['Modification', 'Age', 'FractionIdentifier']\n",
    "if len(IGNORE_COLS) > 0:\n",
    "    freq_df = freq_df[~freq_df['Annotation Type'].isin(IGNORE_COLS)]\n",
    "print(freq_df)\n",
    "# 1. Pivot so that Annotation Type becomes the row index\n",
    "df_plot = freq_df.set_index(\"Annotation Type\")[[\"Annotation Frequency\",\"SDRF Frequency\"]]\n",
    "\n",
    "# 2. Draw the heatmap\n",
    "plt.figure(figsize=(6, 12))                 # adjust size as needed\n",
    "ax = sns.heatmap(\n",
    "    df_plot,\n",
    "    annot=True,      # show the numbers in the cells\n",
    "    fmt=\"d\",         # integer format\n",
    "    linewidths=.5,   # subtle lines between cells\n",
    "    cbar_kws={\"label\": \"Frequency\"},  # color‐bar label\n",
    "    yticklabels=True  # ensure all row labels are shown\n",
    ")\n",
    "\n",
    "# 3. Rotate x-axis tick labels by 45°\n",
    "ax.tick_params(\n",
    "    axis='x',\n",
    "    labelsize=9,\n",
    "    rotation=45\n",
    ")\n",
    "\n",
    "plt.title(\"Annotation vs SDRF Frequencies\")\n",
    "plt.ylabel(\"Annotation Type\")\n",
    "plt.xlabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(annotation_freq_png_outfile, dpi=300)\n",
    "print(f'Saved annotation type frequencies plot to {annotation_freq_png_outfile}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddef341",
   "metadata": {},
   "source": [
    "## Calculate and Plot Performance Metrics\n",
    "Harmonize annotations between two datasets (`A` and `B`) that contain document-level annotations, and evaluate the quality of `B` using `A` as the ground truth. The function aligns semantically or lexically similar annotations via clustering and computes standard classification metrics.\n",
    "\n",
    "### Step-by-Step Overview  \n",
    "**1. Initialization**\n",
    "\n",
    "* Loads a sentence embedding model (`all-MiniLM-L6-v2`) to support the embedding-based harmonization method.\n",
    "* Prepares containers to hold harmonized annotations and evaluation metrics.\n",
    "\n",
    "\n",
    "**2. Identify Shared Documents**\n",
    "\n",
    "* Intersects the document IDs in `A` and `B` to identify a set of shared publications for comparison.\n",
    "\n",
    "\n",
    "**3. Iterate Over Shared Documents and Annotation Categories**\n",
    "\n",
    "For each document and annotation type:\n",
    "\n",
    "**3.1 Extract Unique Annotations**\n",
    "\n",
    "* Gathers the distinct annotations from both datasets for the given category.\n",
    "* Merges them into a unified list (`all_vals`) to establish the comparison space.\n",
    "\n",
    "**3.2 Handle Edge Cases**\n",
    "\n",
    "* If both `A` and `B` contain no annotations, assigns empty harmonized lists and stores NaN for all metrics.\n",
    "* If only one unique annotation exists, skips clustering and directly assigns a single cluster label.\n",
    "\n",
    "\n",
    "**4. Compute Pairwise Distances Between Annotations**\n",
    "\n",
    "Depending on the chosen `method`:\n",
    "\n",
    "* **Embedding**: Uses cosine distance between sentence embeddings. (Slowest - more accurate)  \n",
    "* **RapidFuzz**: Uses 1 minus the normalized string similarity score. (Fastest - less accurate)  \n",
    "\n",
    "\n",
    "**5. Cluster Annotations**\n",
    "\n",
    "* Performs agglomerative clustering based on the precomputed distance matrix.\n",
    "* Groups similar annotations into clusters, where each unique annotation is assigned a cluster ID.\n",
    "\n",
    "\n",
    "**6. Harmonize Original Annotations**\n",
    "\n",
    "* Maps the original annotations from `A` and `B` to their respective cluster IDs using the clustering results.\n",
    "* These harmonized annotations allow consistent comparison despite differing surface forms.\n",
    "\n",
    "\n",
    "**7. Evaluate Harmonized Annotations**\n",
    "\n",
    "Using the harmonized lists:\n",
    "\n",
    "* Identifies all unique cluster IDs across `A` and `B`.\n",
    "* For each cluster ID, constructs binary indicators for presence in `A` and `B`.\n",
    "* Computes the following metrics (macro-averaged):\n",
    "\n",
    "  * **Precision**\n",
    "  * **Recall**\n",
    "  * **F1-score**\n",
    "* Additionally computes **Jaccard similarity** based on the intersection and union of the harmonized cluster ID sets.\n",
    "\n",
    "\n",
    "**8. Store Evaluation Results**\n",
    "\n",
    "* Appends the computed metrics for each document-category pair to a central DataFrame.\n",
    "* Saves this evaluation DataFrame to a CSV file.\n",
    "\n",
    "\n",
    "**9. Save Harmonized Datasets**\n",
    "\n",
    "* Serializes and saves the harmonized annotation dictionaries for `A` and `B` as `.pkl` files using NumPy.\n",
    "\n",
    "\n",
    "**10. Return Outputs**\n",
    "\n",
    "* Returns the harmonized annotation dictionaries for `A` and `B`, along with the full evaluation DataFrame.\n",
    "\n",
    "**11. Plots Resulting Metrics**  \n",
    "\n",
    "* For each annotation type the mean and 95% confidence intervals are calculated and then a table is made with the cells colored according to the magnitude of the metrics.  \n",
    "* Documents where a annotation type was not present in both the SDRF and the minimal annotation test set are assigned precision = recall = f1 = jaccard = float(\"1.0\")  \n",
    "  - There is an argument to not treat these as true negative results so the user can use the `CompleteAbsence = float(\"NaN\")` flag to over ride this behaviour.   \n",
    "* Both the plot file and a .csv containing the plot data are produced.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8084769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "## STEP 3: Harmonize the datasets and evaluate performance ###################\n",
    "# Harmonize the datasets and evaluate performance\n",
    "sdrf_Harm_outfile = os.path.join(OUTPATH, 'harmonized_sdrf.pkl')\n",
    "ann_Harm_outfile = os.path.join(OUTPATH, 'harmonized_ann.pkl')\n",
    "eval_outfile = os.path.join(OUTPATH, 'evaluation_metrics.csv')\n",
    "\n",
    "if os.path.exists(sdrf_Harm_outfile) and os.path.exists(ann_Harm_outfile) and os.path.exists(eval_outfile):\n",
    "    print(f'Harmonized SDRF data already exists at {sdrf_Harm_outfile}. Loading existing data.')\n",
    "    print(f'Harmonized annotation data already exists at {ann_Harm_outfile}. Loading existing data.')\n",
    "    print(f'Evaluation metrics already exist at {eval_outfile}. Loading existing data.')\n",
    "\n",
    "    with open(sdrf_Harm_outfile, 'rb') as f:\n",
    "        Harmonized_sdrf = np.load(f, allow_pickle=True).item()\n",
    "    print(f'Loaded {len(Harmonized_sdrf)} records from {sdrf_Harm_outfile}')\n",
    "\n",
    "    with open(ann_Harm_outfile, 'rb') as f:\n",
    "        Harmonized_ann = np.load(f, allow_pickle=True).item()\n",
    "    print(f'Loaded {len(Harmonized_ann)} records from {ann_Harm_outfile}')\n",
    "\n",
    "    with open(eval_outfile, 'rb') as f:\n",
    "        eval_metrics = pd.read_csv(f)\n",
    "    print(f'Loaded {len(eval_metrics)} records from {eval_outfile}')\n",
    "\n",
    "else:\n",
    "    print(f'Harmonized SDRF data does not exist. Creating new data at {sdrf_Harm_outfile}.')\n",
    "    print(f'Harmonized annotation data does not exist. Creating new data at {ann_Harm_outfile}.')\n",
    "    print(f'Evaluation metrics do not exist. Creating new data at {eval_outfile}.')\n",
    "\n",
    "    Harmonized_sdrf, Harmonized_ann, eval_metrics = Harmonize_and_Evaluate_datasets(A=sdrf_data, B=ann_data, Aoutfile=sdrf_Harm_outfile, Boutfile=ann_Harm_outfile, eval_outfile=eval_outfile, method='RapidFuzz', threshold=0.8, CompleteAbsence=float('1.0'))\n",
    "\n",
    "## Plot the evaluation metrics\n",
    "PlotEvaluationMetrics(eval_metrics, outpath=os.path.join(OUTPATH, 'evaluation_metrics_plot.png'), title='Perforamnce using Fuzzy Matching to\\n Harmonize SDRF and Annotation Data')\n",
    "##############################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
